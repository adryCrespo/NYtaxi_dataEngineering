{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd3ac694",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46bb3640",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SmallTestDataset\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67d9c3bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://24f1e701133a:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>SmallTestDataset</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7791242a03a0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67b99180",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INPUT DATA\n",
    "input_data_g = \"/home/jupyter/proyect/ds_taxi_NY/green_tripdata_2024-01.parquet\"\n",
    "input_data_y = \"/home/jupyter/proyect/ds_taxi_NY/yellow_tripdata_2024-01.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b312104b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIltro registros\n",
    "registros_mes = \"01\"\n",
    "registros_year = \"2025\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "838b89b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCIONES\n",
    "def sumar_missing_per_var(x):\n",
    "    \"\"\"conteo de missing en todas las variables del dataset\"\"\"\n",
    "    \n",
    "    return x.select([F.count( F.when(F.col(c).isNull(),c)  ).alias(c) for c in x.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed58d96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.enabled\",\"True\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8837d7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=true\n",
      "+- == Final Plan ==\n",
      "   *(2) HashAggregate(keys=[date_init_trip#72, PULocationID#5, DOLocationID#6], functions=[sum(total_amount#130), avg(total_amount#130), sum(passenger_count#7L)])\n",
      "   +- ShuffleQueryStage 0\n",
      "      +- Exchange hashpartitioning(date_init_trip#72, PULocationID#5, DOLocationID#6, 1), ENSURE_REQUIREMENTS, [id=#31]\n",
      "         +- *(1) HashAggregate(keys=[date_init_trip#72, PULocationID#5, DOLocationID#6], functions=[partial_sum(total_amount#130), partial_avg(total_amount#130), partial_sum(passenger_count#7L)])\n",
      "            +- *(1) Project [PULocationID#5, DOLocationID#6, passenger_count#7L, round((total_amount#16 + tip_amount#12), 2) AS total_amount#130, date_format(lpep_pickup_datetime#1, yyyy-MM-dd, Some(Etc/UTC)) AS date_init_trip#72]\n",
      "               +- *(1) ColumnarToRow\n",
      "                  +- FileScan parquet [lpep_pickup_datetime#1,PULocationID#5,DOLocationID#6,passenger_count#7L,tip_amount#12,total_amount#16] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/jupyter/proyect/ds_taxi_NY/green_tripdata_2024-01.parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<lpep_pickup_datetime:timestamp,PULocationID:int,DOLocationID:int,passenger_count:bigint,ti...\n",
      "+- == Initial Plan ==\n",
      "   HashAggregate(keys=[date_init_trip#72, PULocationID#5, DOLocationID#6], functions=[sum(total_amount#130), avg(total_amount#130), sum(passenger_count#7L)])\n",
      "   +- Exchange hashpartitioning(date_init_trip#72, PULocationID#5, DOLocationID#6, 1), ENSURE_REQUIREMENTS, [id=#13]\n",
      "      +- HashAggregate(keys=[date_init_trip#72, PULocationID#5, DOLocationID#6], functions=[partial_sum(total_amount#130), partial_avg(total_amount#130), partial_sum(passenger_count#7L)])\n",
      "         +- Project [PULocationID#5, DOLocationID#6, passenger_count#7L, round((total_amount#16 + tip_amount#12), 2) AS total_amount#130, date_format(lpep_pickup_datetime#1, yyyy-MM-dd, Some(Etc/UTC)) AS date_init_trip#72]\n",
      "            +- FileScan parquet [lpep_pickup_datetime#1,PULocationID#5,DOLocationID#6,passenger_count#7L,tip_amount#12,total_amount#16] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/jupyter/proyect/ds_taxi_NY/green_tripdata_2024-01.parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<lpep_pickup_datetime:timestamp,PULocationID:int,DOLocationID:int,passenger_count:bigint,ti...\n",
      "\n",
      "\n",
      "+--------------+------------+------------+------------------+------------------+--------------------+\n",
      "|date_init_trip|PULocationID|DOLocationID| sum(total_amount)| avg(total_amount)|sum(passenger_count)|\n",
      "+--------------+------------+------------+------------------+------------------+--------------------+\n",
      "|    2024-01-01|         236|         239|             25.27|             25.27|                   1|\n",
      "|    2024-01-01|          65|         170| 80.82000000000001|40.410000000000004|                   6|\n",
      "|    2024-01-01|          74|         262|            678.84| 26.10923076923077|                  30|\n",
      "|    2024-01-01|          74|         116|121.08000000000001|20.180000000000003|                   6|\n",
      "|    2024-01-01|          74|         243|183.21999999999997| 30.53666666666666|                   6|\n",
      "|    2024-01-01|          33|         209|             26.25|             26.25|                   1|\n",
      "|    2024-01-01|          74|         238| 809.7099999999999|25.303437499999998|                  44|\n",
      "|    2024-01-01|         166|         239|             77.05|25.683333333333334|                   8|\n",
      "|    2024-01-01|         226|         226|               6.2|               6.2|                   1|\n",
      "|    2024-01-01|           7|         129|             42.84|             21.42|                   2|\n",
      "|    2024-01-01|          42|          75|             44.28|             22.14|                   2|\n",
      "|    2024-01-01|          41|         141|             92.04|30.680000000000003|                   2|\n",
      "|    2024-01-01|         130|         196|             41.28|             41.28|                   1|\n",
      "|    2024-01-01|          74|          69|             30.24|             30.24|                   1|\n",
      "|    2024-01-01|          41|          74|            114.38|11.437999999999999|                  20|\n",
      "|    2024-01-01|          55|         210|              21.6|              21.6|                   1|\n",
      "|    2024-01-01|          41|          42|            178.08| 13.69846153846154|                  18|\n",
      "|    2024-01-01|          42|         151|             15.84|             15.84|                   1|\n",
      "|    2024-01-01|         255|         255|              34.0|              17.0|                   2|\n",
      "|    2024-01-01|          41|          24|              23.6|              11.8|                  11|\n",
      "+--------------+------------+------------+------------------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "21272\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#READ FILE GREEN\n",
    "df_g = spark.read.parquet(input_data_g)\n",
    "\n",
    "#SELECCION VARIABLES GREEN DATASET\n",
    "lista_vars = ['VendorID',\n",
    " 'lpep_pickup_datetime',\n",
    " 'lpep_dropoff_datetime',\n",
    " 'PULocationID',\n",
    " 'DOLocationID',\n",
    " 'passenger_count',\n",
    " 'trip_distance',\n",
    " 'tip_amount',\n",
    " 'total_amount']\n",
    "\n",
    "dfg = df_g.select(lista_vars)\n",
    "\n",
    "\n",
    "# FEATURE duracion_s: duracion del viaje en segundos\n",
    "dfg = dfg.withColumn('duracion_s', F.unix_timestamp(\"lpep_dropoff_datetime\") - F.unix_timestamp('lpep_pickup_datetime') )\\\n",
    "         .withColumn('duracion_s', F.when(  F.col('duracion_s')<0 , F.col('duracion_s')*-1 ).otherwise(F.col('duracion_s')) )\n",
    "\n",
    "# TRANSFORMACION REDONDEO TIEMPO, Y DATE GREEN TAXI\n",
    "dfg = dfg.withColumn(\"date_init_trip\", F.date_format(F.col(\"lpep_pickup_datetime\"), \"yyyy-MM-dd\"))\\\n",
    "         .withColumn( 'hour_init_trip',     F.hour( F.col(\"lpep_pickup_datetime\") ))\\\n",
    "         .drop(F.col('lpep_pickup_datetime'))\\\n",
    "         .drop( F.col('lpep_dropoff_datetime') )\n",
    "         \n",
    "\n",
    "# FEATURE tipoVehiculo: tipo vehiculo\n",
    "dfg = dfg.withColumn('tipoVehiculo', F.lit('Green'))\n",
    "\n",
    "dfg = dfg.withColumn('total_amount', F.round(F.col('total_amount')+F.col('tip_amount'),2)  )\n",
    "\n",
    "# dfg = dfg.repartition(\"date_init_trip\")\n",
    "dfg1 = dfg.groupBy(\"date_init_trip\", \"PULocationID\",\"DOLocationID\").agg(F.sum('total_amount'), F.mean('total_amount'),F.sum(\"passenger_count\") )\n",
    "print(dfg1.rdd.getNumPartitions())\n",
    "dfg1.explain()\n",
    "\n",
    "dfg1.show(20)\n",
    "print(dfg1.count())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d05a207a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'True'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.adaptive.enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "298569d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.driver.extraJavaOptions',\n",
       "  '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED'),\n",
       " ('spark.driver.host', '24f1e701133a'),\n",
       " ('spark.app.submitTime', '1763308527964'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.app.startTime', '1763308528122'),\n",
       " ('spark.sql.warehouse.dir',\n",
       "  'file:/home/jupyter/proyect/NYtaxi_dataEngineering/prueba/spark-warehouse'),\n",
       " ('spark.driver.port', '38181'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED'),\n",
       " ('spark.app.name', 'SmallTestDataset'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.app.id', 'local-1763308529012')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
